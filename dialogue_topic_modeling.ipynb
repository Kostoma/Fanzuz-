{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "with open('ru_stop_words_with_names.txt', encoding='utf-8') as f:\n",
    "    STOPWORDS = f.read().split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# ФРАНЦУЗСКИЕ ТОПИКИ\n",
    "\n",
    "dir = 'fr_corpus_POS_tagged'\n",
    "filepaths = []\n",
    "\n",
    "for i in os.listdir(dir):\n",
    "    filepaths.append(os.path.join(dir, i))  \n",
    "\n",
    "fr_corpus = []\n",
    "fr_tokens = []\n",
    "\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        dialogues = f.read().split('\\n')\n",
    "        for dialogue in dialogues:\n",
    "            dialogue = dialogue.split()\n",
    "            dialogue_nouns = []\n",
    "            for word in dialogue:\n",
    "                word = word.split('_')\n",
    "                if word[1] == 'NOUN':\n",
    "                    dialogue_nouns.append(word[0])\n",
    "            if dialogue_nouns:\n",
    "                fr_tokens.extend(dialogue_nouns)\n",
    "                dialogue_nouns = ' '.join(dialogue_nouns)\n",
    "                fr_corpus.append(dialogue_nouns)\n",
    "\n",
    "fr_lemmata = set(fr_tokens)\n",
    "print(f'Количество токенов: {len(fr_tokens)}')\n",
    "print(f'Количество лемм: {len(fr_lemmata)}')\n",
    "print(f'Количество документов в корпусе: {len(fr_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# РУССКИЕ ТОПИКИ\n",
    "\n",
    "dir = 'rus_corpus_POS_tagged'\n",
    "filepaths = []\n",
    "\n",
    "for i in os.listdir(dir):\n",
    "    filepaths.append(os.path.join(dir, i))  \n",
    "\n",
    "rus_corpus = []\n",
    "rus_tokens = []\n",
    "\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        dialogues = f.read().split('\\n')\n",
    "        for dialogue in dialogues:\n",
    "            dialogue = dialogue.split()\n",
    "            dialogue_nouns = []\n",
    "            for word in dialogue:\n",
    "                word = word.split('_')\n",
    "                if word[1] == 'NOUN':\n",
    "                    dialogue_nouns.append(word[0])\n",
    "            if dialogue_nouns:\n",
    "                rus_tokens.extend(dialogue_nouns)\n",
    "                dialogue_nouns = ' '.join(dialogue_nouns)\n",
    "                rus_corpus.append(dialogue_nouns)\n",
    "\n",
    "rus_lemmata = set(rus_tokens)\n",
    "print(f'Количество токенов: {len(rus_tokens)}')\n",
    "print(f'Количество лемм: {len(rus_lemmata)}')\n",
    "print(f'Количество документов в корпусе: {len(rus_corpus)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Количество токенов: 60592\n",
      "Количество лемм: 11353\n",
      "Количество документов в корпусе: 6442\n",
      "Количество стоп-слов: 52698\n",
      "не\n",
      "он\n",
      "отвечать\n",
      "за\n",
      "и\n",
      "говорить\n",
      "чтò\n",
      "и\n",
      "что\n",
      "но\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# РУССКИЕ ТОПИКИ БЕЗ POS tag\n",
    "\n",
    "dir = 'rus_corpus_POS_tagged'\n",
    "filepaths = []\n",
    "\n",
    "for i in os.listdir(dir):\n",
    "    filepaths.append(os.path.join(dir, i))  \n",
    "\n",
    "rus_corpus = []\n",
    "rus_tokens = []\n",
    "stop_word_count = 0\n",
    "stop_words_found = []\n",
    "\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        dialogues = f.read().split('\\n')\n",
    "        for dialogue in dialogues:\n",
    "            dialogue = dialogue.split()\n",
    "            dialogue_cleaned = []\n",
    "            for word in dialogue:\n",
    "                if word.split('_')[0] not in STOPWORDS:\n",
    "                    dialogue_cleaned.append(word)\n",
    "                else:\n",
    "                    stop_word_count += 1\n",
    "                    stop_words_found.append(word.split('_')[0])\n",
    "            rus_tokens.extend(dialogue_cleaned)\n",
    "            dialogue_cleaned = ' '.join(dialogue_cleaned)\n",
    "            rus_corpus.append(dialogue_cleaned)\n",
    "                \n",
    "rus_lemmata = set(rus_tokens)\n",
    "print(f'Количество токенов: {len(rus_tokens)}')\n",
    "print(f'Количество лемм: {len(rus_lemmata)}')\n",
    "print(f'Количество документов в корпусе: {len(rus_corpus)}')\n",
    "print(f'Количество стоп-слов: {stop_word_count}')\n",
    "for i in range(10):\n",
    "    print(random.choice(stop_words_found))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Количество токенов: 5193\n",
      "Количество лемм: 2352\n",
      "Количество документов в корпусе: 1027\n",
      "Количество стоп-слов: 3736\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# ФРАНЦУЗСКИЕ ТОПИКИ БЕЗ POS tag\n",
    "\n",
    "dir = 'fr_corpus_POS_tagged'\n",
    "filepaths = []\n",
    "\n",
    "for i in os.listdir(dir):\n",
    "    filepaths.append(os.path.join(dir, i))  \n",
    "\n",
    "fr_corpus = []\n",
    "fr_tokens = []\n",
    "stop_word_count = 0\n",
    "\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        dialogues = f.read().split('\\n')\n",
    "        for dialogue in dialogues:\n",
    "            dialogue = dialogue.split()\n",
    "            dialogue_cleaned = []\n",
    "            for word in dialogue:\n",
    "                if word.split('_')[0] not in STOPWORDS:\n",
    "                    dialogue_cleaned.append(word)\n",
    "                else:\n",
    "                    stop_word_count += 1\n",
    "            fr_tokens.extend(dialogue_cleaned)\n",
    "            dialogue_cleaned = ' '.join(dialogue_cleaned)\n",
    "            fr_corpus.append(dialogue_cleaned)\n",
    "                \n",
    "fr_lemmata = set(fr_tokens)\n",
    "print(f'Количество токенов: {len(fr_tokens)}')\n",
    "print(f'Количество лемм: {len(fr_lemmata)}')\n",
    "print(f'Количество документов в корпусе: {len(fr_corpus)}')\n",
    "print(f'Количество стоп-слов: {stop_word_count}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus = fr_corpus + rus_corpus\n",
    "tokens = fr_tokens + rus_tokens\n",
    "lemmata = set.union(fr_lemmata, rus_lemmata)\n",
    "\n",
    "print(f'Количество токенов: {len(tokens)}')\n",
    "print(f'Количество лемм: {len(lemmata)}')\n",
    "print(f'Количество документов в корпусе: {len(corpus)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['надо_PRED выдумать_INFN', 'завтра_ADVB милый_ADJF', 'бояться_VERB ангел_NOUN', 'господин_NOUN посольство_NOUN', 'самый_ADJF замечательный_ADJF женщина_NOUN петербург_NOUN', 'серьёзно_ADVB', 'паж_NOUN', 'синить_VERB чулок_NOUN', 'жена_NOUN', 'вполне_ADVB порядочно_ADVB']\n",
      "молодой_ADJF\n",
      "вена_NOUN\n",
      "ж_CONJ\n",
      "свойственно_ADVB\n",
      "презирать_VERB\n",
      "королева_NOUN\n",
      "взять_VERB\n",
      "корабль_NOUN\n",
      "мимолётность_NOUN\n",
      "каков_ADJS\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(fr_corpus[:10])\n",
    "for i in range(10):\n",
    "    pass\n",
    "    print(random.choice(fr_tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-b08bbab95017>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m nmf = LatentDirichletAllocation(n_components=no_topics, \n\u001b[1;32m---> 15\u001b[1;33m           \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nndsvd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m           )\n\u001b[0;32m     17\u001b[0m \u001b[0mnmf_tfidf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_tfidf_vectorized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'alpha'"
     ],
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'alpha'",
     "output_type": "error"
    }
   ],
   "source": [
    "no_features = 2300\n",
    "no_topics = 10\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    # min_df=3, \n",
    "    # stop_words=STOPWORDS, \n",
    "    max_features=no_features\n",
    ")\n",
    "data_tfidf_vectorized = tfidf_vectorizer.fit_transform(fr_corpus)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "nmf = NMF(n_components=no_topics, \n",
    "          random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd'\n",
    "          )\n",
    "nmf_tfidf_model = nmf.fit(data_tfidf_vectorized)\n",
    "print(nmf_tfidf_model.transform(data_tfidf_vectorized).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(6442, 10)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "no_features = 11000\n",
    "no_topics = 10\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    # min_df=3, \n",
    "    # stop_words=STOPWORDS, \n",
    "    max_features=no_features\n",
    ")\n",
    "data_tfidf_vectorized = tfidf_vectorizer.fit_transform(rus_corpus)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "nmf = LatentDirichletAllocation(n_components=no_topics, \n",
    "          random_state=1,\n",
    "          )\n",
    "nmf_tfidf_model = nmf.fit(data_tfidf_vectorized)\n",
    "print(nmf_tfidf_model.transform(data_tfidf_vectorized).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_topics(model, feature_names, top_n=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "            print(\"Topic %d:\" % (topic_idx))\n",
    "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-top_n - 1:-1]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def print_topics(model, feature_names, top_n=50):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f'Topic {topic_idx}')\n",
    "        feature_list = [feature_names[i] for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        output = [x.split('_')[0] for x in feature_list if x.split('_')[1] == 'noun'][:10]\n",
    "        print(' '.join(output))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Topic 0\n",
      "княжна время герой вера матушка героизм сомнение вид особа здоровье\n",
      "Topic 1\n",
      "величество резерв высокоблагородие император семейство жизнь сын мнение высочество заряд\n",
      "Topic 2\n",
      "друг здоровье интерес молодец покров полчаса мужчина удар богиня ножка\n",
      "Topic 3\n",
      "император король свет наполеон александр господин герцог австриец кузина князь\n",
      "Topic 4\n",
      "дружок мужчина истина история испанец источник камень казак кавалерист кавалер\n",
      "Topic 5\n",
      "человек достоинство князь ум господин молодая великое сын граф доктор\n",
      "Topic 6\n",
      "батюшка истина испанец история кабинет камень казак кавалерист кавалер итти\n",
      "Topic 7\n",
      "государь брат герцог приём сомнение дело император отсутствие париж величество\n",
      "Topic 8\n",
      "женщина петербург вино перл подруга мужчина княжна кавалерист кавалер кабинет\n",
      "Topic 9\n",
      "бог слово князь поход молитва повышение история истина источник итальянец\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print_topics(nmf_tfidf_model, tfidf_feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "fr_visualization = pyLDAvis.sklearn.prepare(nmf_tfidf_model, data_tfidf_vectorized, tfidf_vectorizer)\n",
    "pyLDAvis.save_html(fr_visualization, 'rus_visualization.html')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}